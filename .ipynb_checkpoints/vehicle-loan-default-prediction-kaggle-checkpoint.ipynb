{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/vehicle-loan-default-prediction/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17640/2418324039.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[0mtest_csv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/kaggle/input/vehicle-loan-default-prediction/test.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_csv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'LOAN_DEFAULT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/vehicle-loan-default-prediction/train.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, auc, roc_curve, roc_auc_score, balanced_accuracy_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dt = datetime.today()\n",
    "\n",
    "\n",
    "def removeOutlier(dataSet, feature):    \n",
    "    q1=dataSet[feature].quantile(0.25)\n",
    "    q3=dataSet[feature].quantile(0.75)\n",
    "    IQR=q3-q1\n",
    "    lowerLimit = q1 - 1.5 * IQR\n",
    "    UpperLimit = q3 + 1.5 * IQR \n",
    "    dataSet = dataSet[dataSet[feature]< UpperLimit]\n",
    "    dataSet = dataSet[dataSet[feature]> lowerLimit]\n",
    "    return dataSet\n",
    "\n",
    "def modelEvaluation(model, X_test, y_train,y_test, y_pred,cols=None):\n",
    "    print(\"Accurancy: {:.3f}\".format(accuracy_score(y_test, y_pred)))\n",
    "    print(\"ROC AUC Score: {:.3f}\".format(roc_auc_score(y_test, y_pred)))\n",
    "    print(\"F1 Score:: {:.3f} \".format(f1_score(y_test, y_pred)))\n",
    "    print(\"Balanced Accurancy Score:: {:.3f} \".format(balanced_accuracy_score(y_test, y_pred)))\n",
    "    print('\\n clasification report:\\n', classification_report(y_test,y_pred))\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    prediction_probabilities = model.predict_proba(X_test[cols])[:,1]\n",
    "    fpr , tpr , thresholds = roc_curve(y_test,prediction_probabilities)\n",
    "    ax.plot(fpr,tpr,label = [\"Area under curve : \",auc(fpr,tpr)],linewidth=2,linestyle=\"dotted\")\n",
    "    ax.plot([0,1],[0,1],linewidth=2,linestyle=\"dashed\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"ROC-CURVE and AREA UNDER CURVE\")\n",
    "    ax.set_facecolor(\"k\")\n",
    "    \n",
    "def calculate_age(row):\n",
    "    try:\n",
    "        yrs, mon = tuple(row['AVERAGE_ACCT_AGE'].split(' '))\n",
    "        age = round(float(yrs.replace('yrs', '')) + float(mon.replace('mon', ''))/12, 2)\n",
    "        row['AVERAGE_ACCT_AGE'] = age\n",
    "\n",
    "        yrs, mon = tuple(row['CREDIT_HISTORY_LENGTH'].split(' '))\n",
    "        age = round(float(yrs.replace('yrs', '')) + float(mon.replace('mon', ''))/12, 2)\n",
    "        row['CREDIT_HISTORY_LENGTH'] = age\n",
    "    except Exception as e:\n",
    "        print(row, e)\n",
    "        raise e\n",
    "    return row\n",
    "\n",
    "\n",
    "def binning_by_depth_factor(df_column, factor):\n",
    "    divs, max_da, min_da= round(np.sqrt(len(df_column))/factor), df_column.max(), df_column.min()\n",
    "    step = (max_da - min_da)/divs\n",
    "    return (df_column/step).astype(int)*divs\n",
    "\n",
    "raw_df = pd.read_csv(\"data.csv\")\n",
    "training_csv = '/kaggle/input/vehicle-loan-default-prediction/train.csv'\n",
    "test_csv = '/kaggle/input/vehicle-loan-default-prediction/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(training_csv)\n",
    "train_df = train_df[~train_df['LOAN_DEFAULT'].isna()]\n",
    "print(train_df.columns)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['LOAN_DEFAULT'].astype(bool).value_counts().plot.pie()\n",
    "train_df['LOAN_DEFAULT'].astype(bool).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The pie chart of the target class shows the skewness of data, leaning towards False and thus needs to be accounted for. \n",
    ">\n",
    "> Will be applying Resampling Technique to balance the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before Resampling:')\n",
    "print(train_df['LOAN_DEFAULT'].value_counts())\n",
    "df_majority = train_df[train_df['LOAN_DEFAULT']==0]\n",
    "df_minority = train_df[train_df['LOAN_DEFAULT']==1]\n",
    "\n",
    "\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,                  # sample with replacement\n",
    "                                 n_samples=len(df_majority),    # to match majority class\n",
    "                                 random_state=123)              # reproducible results\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "print('After Resampling:')\n",
    "print(df_upsampled['LOAN_DEFAULT'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = ['MOBILENO_AVL_FLAG', 'AADHAR_FLAG', 'PAN_FLAG', 'VOTERID_FLAG', 'DRIVING_FLAG', 'PASSPORT_FLAG']\n",
    "fig, axs = plt.subplots(len(flags), 2, figsize=(10, 10))\n",
    "\n",
    "for _i, flag in enumerate(flags):\n",
    "    for i, (vals, group) in enumerate(train_df.groupby([flag])):\n",
    "        group.groupby(['LOAN_DEFAULT']).size().plot.pie(ax=axs[_i][i])\n",
    "        axs[_i][i].set_title(str(flag)+' '+str(vals))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = ['DISBURSED_AMOUNT', 'ASSET_COST', 'LTV']\n",
    "fig, axs = plt.subplots(len(cols), figsize=(10, 5*len(cols)))\n",
    "\n",
    "for j, col in enumerate(cols):\n",
    "    for i, (val, group) in enumerate(train_df.groupby('LOAN_DEFAULT')):\n",
    "        column = binning_by_depth_factor(group[col], 10) \n",
    "        column.value_counts().to_frame('counts').reset_index().sort_values('index').plot(x='index', y='counts', label=val, ax=axs[j])\n",
    "        axs[j].set_xlabel(col)\n",
    "        axs[j].set_ylabel('counts')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Disbursed amount and asset cost in both both classes showing gaussian distribution but with different distribution frequencies. \n",
    ">\n",
    "> This shows that the defaulters are defaulting mostly in lesser amounts of loan and for lesser asset costs.\n",
    ">\n",
    "> Additionally as expected, lower LTV value customers are also defaulting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['DOB'] = pd.to_datetime(train_df['DATE_OF_BIRTH'], format='%d-%m-%Y', errors='coerce')\n",
    "train_df['DD'] = pd.to_datetime(train_df['DISBURSAL_DATE'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "\n",
    "train_df = pd.DataFrame(train_df[~train_df['DOB'].isna()])\n",
    "train_df = pd.DataFrame(train_df[~train_df['DD'].isna()])\n",
    "train_df['APPLICANT_AGE'] = ((dt - train_df['DOB']).apply(lambda x: float(x.days)) / 365.0)\n",
    "train_df['DISBURSAL_AGE'] = ((dt - train_df['DD']).apply(lambda x: float(x.days)) / 365.0)\n",
    "\n",
    "cols = ['APPLICANT_AGE', 'DISBURSAL_AGE']\n",
    "\n",
    "for j, col in enumerate(cols):\n",
    "    fig, axs = plt.subplots(1, len(cols), figsize=(20, 5), sharey=True)\n",
    "    for i, (val, group) in enumerate(train_df.groupby('LOAN_DEFAULT')):\n",
    "        group[col].plot.box( ax=axs[i], label=val)\n",
    "        axs[i].set_title(col)\n",
    "        axs[i].set_xlabel(\"Loan Default: %s\"%bool(val))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As seen from the box plots, Applicant Age seems to have almost no impact on the defaulting, similar behaviour is seen in Disbursal Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby('PERFORM_CNS_SCORE_DESCRIPTION').agg({'PERFORM_CNS_SCORE':[np.min, np.max]}).sort_values(('PERFORM_CNS_SCORE', 'amin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PERFORM_CNS_SCORE_DESCRIPTION seems to be a bucket label for PERFORM_CNS_SCORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[~train_df['LOAN_DEFAULT'].isna()]\n",
    "\n",
    "total_records = len(train_df)\n",
    "\n",
    "analysis = []\n",
    "\n",
    "train_df = train_df.apply(calculate_age, axis=1)\n",
    "train_df['AVERAGE_ACCT_AGE'] = train_df['AVERAGE_ACCT_AGE'].astype(float)\n",
    "train_df['CREDIT_HISTORY_LENGTH'] = train_df['CREDIT_HISTORY_LENGTH'].astype(float)\n",
    "\n",
    "train_df['AADHAR_FLAG'] = train_df['AADHAR_FLAG'].astype(bool)\n",
    "train_df['PAN_FLAG'] = train_df['PAN_FLAG'].astype(bool)\n",
    "train_df['VOTERID_FLAG'] = train_df['VOTERID_FLAG'].astype(bool)\n",
    "train_df['DRIVING_FLAG'] = train_df['DRIVING_FLAG'].astype(bool)\n",
    "train_df['PASSPORT_FLAG'] = train_df['PASSPORT_FLAG'].astype(bool)\n",
    "\n",
    "train_df['DATE_OF_BIRTH'] = pd.to_datetime(train_df['DATE_OF_BIRTH'])\n",
    "train_df['DISBURSAL_DATE'] = pd.to_datetime(train_df['DISBURSAL_DATE'])\n",
    "\n",
    "train_df['APPLICANT_AGE'] = ((dt - train_df['DATE_OF_BIRTH']) / 365).apply(lambda x: float(x.days))\n",
    "train_df['DISBURSAL_AGE'] = ((dt - train_df['DISBURSAL_DATE']) / 365).apply(lambda x: float(x.days))\n",
    "\n",
    "train_df = train_df.drop('DATE_OF_BIRTH', axis=1)\n",
    "train_df = train_df.drop('DISBURSAL_DATE', axis=1)\n",
    "train_df = train_df.drop('PERFORM_CNS_SCORE_DESCRIPTION', axis=1)\n",
    "train_df = train_df.drop('MOBILENO_AVL_FLAG', axis=1)\n",
    "train_df = train_df.drop('UNIQUEID', axis=1)\n",
    "\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = removeOutlier(train_df, 'DISBURSED_AMOUNT')\n",
    "train_df = removeOutlier(train_df, 'ASSET_COST')\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(train_df.corr(), ax=ax, vmin=0, vmax=1, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = train_df.corr()\n",
    "for col in corr_df.columns[:]:\n",
    "    correlated_fields = list(filter(lambda x: x!=col, corr_df[(np.abs(corr_df[col])>.75)][col].index.values))\n",
    "    correlated_values = list(filter(lambda x: x!=col, corr_df[(np.abs(corr_df[col])>.75)][col].values))\n",
    "    if correlated_fields:\n",
    "        print(col, ':', ', '.join(map(str, zip(correlated_fields, correlated_values))), sep='\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From the Correlation Graph and the code filter above, we see that the below fields are highly correlated\n",
    ">\n",
    "> -\tPRI_NO_OF_ACCTS\t:\tPRI_ACTIVE_ACCTS\n",
    "> -\tPRI_ACTIVE_ACCTS\t:\tPRI_NO_OF_ACCTS\n",
    "> -\tPRI_CURRENT_BALANCE\t:\tPRI_SANCTIONED_AMOUNT, PRI_DISBURSED_AMOUNT\n",
    "> -\tPRI_SANCTIONED_AMOUNT\t:\tPRI_CURRENT_BALANCE, PRI_DISBURSED_AMOUNT\n",
    "> -\tPRI_DISBURSED_AMOUNT\t:\tPRI_CURRENT_BALANCE, PRI_SANCTIONED_AMOUNT\n",
    "> -\tSEC_NO_OF_ACCTS\t:\tSEC_ACTIVE_ACCTS\n",
    "> -\tSEC_ACTIVE_ACCTS\t:\tSEC_NO_OF_ACCTS\n",
    "> -\tSEC_CURRENT_BALANCE\t:\tSEC_SANCTIONED_AMOUNT, SEC_DISBURSED_AMOUNT\n",
    "> -\tSEC_SANCTIONED_AMOUNT\t:\tSEC_CURRENT_BALANCE, SEC_DISBURSED_AMOUNT\n",
    "> -\tSEC_DISBURSED_AMOUNT\t:\tSEC_CURRENT_BALANCE, SEC_SANCTIONED_AMOUNT\n",
    "> -\tAVERAGE_ACCT_AGE\t:\tCREDIT_HISTORY_LENGTH\n",
    "> -\tCREDIT_HISTORY_LENGTH\t:\tAVERAGE_ACCT_AGE\n",
    ">\n",
    "> From the above, we can remove the below columns from the feature set\n",
    "> - PRI_NO_OF_ACCTS\n",
    "> - PRI_SANCTIONED_AMOUNT\n",
    "> - SEC_SANCTIONED_AMOUNT\n",
    "> - AVERAGE_ACCT_AGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['PRI_NO_OF_ACCTS', 'PRI_SANCTIONED_AMOUNT', 'SEC_SANCTIONED_AMOUNT', 'AVERAGE_ACCT_AGE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_counts_df = train_df.groupby(['STATE_ID', 'LOAN_DEFAULT']).size().to_frame('counts').reset_index()\n",
    "\n",
    "states_counts =[(ld, group) for ld, group in state_counts_df.groupby(['LOAN_DEFAULT'])]\n",
    "\n",
    "for ld, state_df in states_counts:\n",
    "    state_df.columns=['STATE_ID', 'LOAN_DEFAULT', 'COUNT_%s'%bool(ld)]\n",
    "    \n",
    "states_counts_df = states_counts[0][1].merge(states_counts[1][1], on='STATE_ID')\n",
    "states_counts_df = states_counts_df.drop(['LOAN_DEFAULT_x', 'LOAN_DEFAULT_y'], axis=1)\n",
    "states_counts_df['total']= states_counts_df['COUNT_False']+ states_counts_df['COUNT_True']\n",
    "states_counts_df['COUNT_False']= states_counts_df['COUNT_False']/ states_counts_df['total']\n",
    "states_counts_df['COUNT_True']= states_counts_df['COUNT_True']/ states_counts_df['total']\n",
    "states_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting and filling back in the EMPLOYMENT_TYPE Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Employment Type Blanks: %d'% len(train_df[train_df['EMPLOYMENT_TYPE'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Using a RandomForestClassifier to fill in Employment Type Field for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['EMPLOYMENT_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [column for column in train_df.columns if column not in ['LOAN_DEFAULT', 'EMPLOYMENT_TYPE', 'EMPLOYMENT_TYPE_code', 'DD', 'DOB']]\n",
    "target_variable = 'EMPLOYMENT_TYPE'\n",
    "\n",
    "emp_type_df = pd.DataFrame(train_df[~train_df[target_variable].isna()])\n",
    "\n",
    "df_majority = emp_type_df[emp_type_df['EMPLOYMENT_TYPE']=='Self employed']\n",
    "df_minority = emp_type_df[emp_type_df['EMPLOYMENT_TYPE']=='Salaried']\n",
    "\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,                  # sample with replacement\n",
    "                                 n_samples=len(df_majority),    # to match majority class\n",
    "                                 random_state=123)              # reproducible results\n",
    "\n",
    "emp_type_df = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "emp_type_df['%s_code'%target_variable], employment_type_map = emp_type_df[target_variable].factorize()\n",
    "\n",
    "X_df = emp_type_df[feature_columns]\n",
    "Y_df = emp_type_df['%s_code'%target_variable]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_df, Y_df)\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_predict = classifier.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_predict), accuracy_score(y_test, y_predict), f1_score(y_test, y_predict))\n",
    "\n",
    "\n",
    "to_pred_df = pd.DataFrame(train_df[train_df[target_variable].isna()])\n",
    "to_pred_df['%s_code'%target_variable] = -1\n",
    "to_pred_df['%s_code'%target_variable] = classifier.predict(to_pred_df[feature_columns])\n",
    "train_df[train_df[target_variable].isna()][target_variable] = to_pred_df['%s_code'%target_variable].apply(lambda x: employment_type_map[x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Transformation Function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_employement_type(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    feature_columns = [column for column in df.columns if column not in ['LOAN_DEFAULT', 'EMPLOYMENT_TYPE', 'EMPLOYMENT_TYPE_code', 'DD', 'DOB']]\n",
    "    target_variable = 'EMPLOYMENT_TYPE'\n",
    "\n",
    "    emp_type_df = pd.DataFrame(df[~df[target_variable].isna()])\n",
    "    df_majority = emp_type_df[emp_type_df['EMPLOYMENT_TYPE']=='Self employed']\n",
    "    df_minority = emp_type_df[emp_type_df['EMPLOYMENT_TYPE']=='Salaried']\n",
    "\n",
    "    df_minority_upsampled = resample(df_minority, \n",
    "                                     replace=True,                  # sample with replacement\n",
    "                                     n_samples=len(df_majority),    # to match majority class\n",
    "                                     random_state=123)              # reproducible results\n",
    "\n",
    "    emp_type_df = pd.concat([df_majority, df_minority_upsampled])\n",
    "    emp_type_df['%s_code'%target_variable], employment_type_map = emp_type_df[target_variable].factorize()\n",
    "\n",
    "    X_df = emp_type_df[feature_columns]\n",
    "    Y_df = emp_type_df['%s_code'%target_variable]\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_df, Y_df)\n",
    "    classifier = RandomForestClassifier()\n",
    "    classifier.fit(x_train, y_train)\n",
    "    y_predict = classifier.predict(x_test)\n",
    "    print(confusion_matrix(y_test, y_predict), accuracy_score(y_test, y_predict), f1_score(y_test, y_predict))\n",
    "\n",
    "    to_pred_df = pd.DataFrame(df[df[target_variable].isna()])\n",
    "    to_pred_df['%s_code'%target_variable] = -1\n",
    "    to_pred_df['%s_code'%target_variable] = classifier.predict(to_pred_df[feature_columns])\n",
    "    df[df[target_variable].isna()][target_variable] = to_pred_df['%s_code'%target_variable].apply(lambda x: employment_type_map[x])\n",
    "    return df\n",
    "\n",
    "def prepare(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    df = df.apply(calculate_age, axis=1)\n",
    "    df = removeOutlier(df, 'DISBURSED_AMOUNT')\n",
    "    df = removeOutlier(df, 'ASSET_COST')\n",
    "\n",
    "    # Set data types\n",
    "    df['AVERAGE_ACCT_AGE'] = df['AVERAGE_ACCT_AGE'].astype(float)\n",
    "    df['CREDIT_HISTORY_LENGTH'] = df['CREDIT_HISTORY_LENGTH'].astype(float)\n",
    "    df['AADHAR_FLAG'] = df['AADHAR_FLAG'].astype(bool)\n",
    "    df['PAN_FLAG'] = df['PAN_FLAG'].astype(bool)\n",
    "    df['VOTERID_FLAG'] = df['VOTERID_FLAG'].astype(bool)\n",
    "    df['DRIVING_FLAG'] = df['DRIVING_FLAG'].astype(bool)\n",
    "    df['PASSPORT_FLAG'] = df['PASSPORT_FLAG'].astype(bool)\n",
    "    \n",
    "    # Parse Dates\n",
    "    df['DATE_OF_BIRTH'] = pd.to_datetime(df['DATE_OF_BIRTH'])\n",
    "    df['DISBURSAL_DATE'] = pd.to_datetime(df['DISBURSAL_DATE'])\n",
    "    \n",
    "    df['APPLICANT_AGE'] = ((dt - df['DATE_OF_BIRTH']) / 365).apply(lambda x: float(x.days))\n",
    "    df['DISBURSAL_AGE'] = ((dt - df['DISBURSAL_DATE']) / 365).apply(lambda x: float(x.days))\n",
    "    emp_type_df = pd.get_dummies(df['EMPLOYMENT_TYPE'], prefix='EMPLOYMENT_TYPE', drop_first=True)\n",
    "    df[emp_type_df.columns.tolist()] = emp_type_df\n",
    "    \n",
    "    # Dropping Columns\n",
    "    columns_to_drop = ['DATE_OF_BIRTH', 'DISBURSAL_DATE', 'PERFORM_CNS_SCORE_DESCRIPTION','MOBILENO_AVL_FLAG','UNIQUEID', 'PRI_NO_OF_ACCTS', 'PRI_SANCTIONED_AMOUNT', 'SEC_SANCTIONED_AMOUNT', 'AVERAGE_ACCT_AGE']\n",
    "    df = df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "    #Fill back Employment Type\n",
    "    df = predict_employement_type(df)\n",
    "    df = df.drop('EMPLOYMENT_TYPE', axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = pd.read_csv(training_csv)\n",
    "train_df = prepare(train_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model: RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['DISBURSED_AMOUNT', 'ASSET_COST', 'LTV', 'BRANCH_ID', 'SUPPLIER_ID',\n",
    "       'MANUFACTURER_ID', 'CURRENT_PINCODE_ID', 'STATE_ID',\n",
    "       'EMPLOYEE_CODE_ID', 'AADHAR_FLAG', 'PAN_FLAG', 'VOTERID_FLAG',\n",
    "       'DRIVING_FLAG', 'PASSPORT_FLAG', 'PERFORM_CNS_SCORE',\n",
    "       'PRI_ACTIVE_ACCTS', 'PRI_OVERDUE_ACCTS', 'PRI_CURRENT_BALANCE',\n",
    "       'PRI_DISBURSED_AMOUNT', 'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS',\n",
    "       'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE', 'SEC_DISBURSED_AMOUNT',\n",
    "       'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT', 'NEW_ACCTS_IN_LAST_SIX_MONTHS',\n",
    "       'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'CREDIT_HISTORY_LENGTH',\n",
    "       'NO_OF_INQUIRIES',  'APPLICANT_AGE', 'DISBURSAL_AGE',\n",
    "       'EMPLOYMENT_TYPE_Self employed']\n",
    "target_column = 'LOAN_DEFAULT'\n",
    "\n",
    "print('Before Resampling:')\n",
    "print(train_df['LOAN_DEFAULT'].value_counts())\n",
    "df_majority = train_df[train_df['LOAN_DEFAULT']==0]\n",
    "df_minority = train_df[train_df['LOAN_DEFAULT']==1]\n",
    "\n",
    "\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,                  # sample with replacement\n",
    "                                 n_samples=len(df_majority),    # to match majority class\n",
    "                                 random_state=123)              # reproducible results\n",
    "train_df = pd.concat([df_majority, df_minority_upsampled])\n",
    "print('After Resampling:')\n",
    "print(train_df['LOAN_DEFAULT'].value_counts())\n",
    "\n",
    "\n",
    "X_df = train_df[feature_columns]\n",
    "Y_df = train_df[target_column]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_df, Y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier  = LogisticRegression()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_predict = classifier.predict(x_test)\n",
    "modelEvaluation(classifier,x_test, y_train, y_test, y_predict, x_train.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier  = DecisionTreeClassifier()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_predict = classifier.predict(x_test)\n",
    "modelEvaluation(classifier,x_test, y_train, y_test, y_predict, x_train.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_predict = classifier.predict(x_test)\n",
    "modelEvaluation(classifier,x_test, y_train, y_test, y_predict, x_train.columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
